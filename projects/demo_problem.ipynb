{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country  Age    Salary  Purchase\n",
      "0  Canada   19  807572.0     False\n",
      "1   India   32  176316.0      True\n",
      "2  Brazil   23  649176.0     False\n",
      "3     USA   19  128246.0      True\n",
      "4   India   50       NaN      True\n"
     ]
    }
   ],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('synthetic_data.csv')\n",
    "\n",
    "# check the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Canada' 19 807572.0]\n",
      " ['India' 32 176316.0]\n",
      " ['Brazil' 23 649176.0]\n",
      " ['USA' 19 128246.0]\n",
      " ['India' 50 nan]\n",
      " ['Brazil' 30 780800.0]\n",
      " ['India' 46 647889.0]\n",
      " ['USA' 66 874962.0]\n",
      " ['France' 39 321369.0]\n",
      " ['India' 66 382944.0]]\n"
     ]
    }
   ],
   "source": [
    "# load the data into numpy arrays\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, 3].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Canada' 19.0 807572.0]\n",
      " ['India' 32.0 176316.0]\n",
      " ['Brazil' 23.0 649176.0]\n",
      " ['USA' 19.0 128246.0]\n",
      " ['India' 50.0 529919.3333333334]\n",
      " ['Brazil' 30.0 780800.0]\n",
      " ['India' 46.0 647889.0]\n",
      " ['USA' 66.0 874962.0]\n",
      " ['France' 39.0 321369.0]\n",
      " ['India' 66.0 382944.0]]\n"
     ]
    }
   ],
   "source": [
    "# perform preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# assuming x is your data with missing values\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Dataset:\n",
      "[[0.0 1.0 0.0 0.0 0.0 19.0 807572.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 32.0 176316.0]\n",
      " [1.0 0.0 0.0 0.0 0.0 23.0 649176.0]\n",
      " [0.0 0.0 0.0 0.0 1.0 19.0 128246.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 50.0 529919.3333333334]\n",
      " [1.0 0.0 0.0 0.0 0.0 30.0 780800.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 46.0 647889.0]\n",
      " [0.0 0.0 0.0 0.0 1.0 66.0 874962.0]\n",
      " [0.0 0.0 1.0 0.0 0.0 39.0 321369.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 66.0 382944.0]]\n",
      "\n",
      "Encoded Dependent Variable:\n",
      "[0 1 0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Extract the categorical feature (Country) from the encoding \n",
    "countries = X[:, 0].reshape(-1, 1)\n",
    "\n",
    "# Applying OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "countries = onehotencoder.fit_transform(countries).toarray()\n",
    "\n",
    "# Combine the encoded countries with the rest of the data\n",
    "X_encoded = np.concatenate((countries, X[:, 1:]), axis=1) # axis=1 means concatenate columns\n",
    "\n",
    "print(\"Encoded Dataset:\")\n",
    "print(X_encoded)\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "print(\"\\nEncoded Dependent Variable:\")\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set:\n",
      "[[1.0 0.0 0.0 0.0 0.0 30.0 780800.0]\n",
      " [0.0 1.0 0.0 0.0 0.0 19.0 807572.0]\n",
      " [0.0 0.0 0.0 0.0 1.0 66.0 874962.0]\n",
      " [1.0 0.0 0.0 0.0 0.0 23.0 649176.0]\n",
      " [0.0 0.0 0.0 0.0 1.0 19.0 128246.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 46.0 647889.0]\n",
      " [0.0 0.0 0.0 1.0 0.0 66.0 382944.0]\n",
      " [0.0 0.0 1.0 0.0 0.0 39.0 321369.0]]\n",
      "[0 0 1 0 1 0 1 1]\n",
      "\n",
      "Test Set:\n",
      "[[0.0 0.0 0.0 1.0 0.0 50.0 529919.3333333334]\n",
      " [0.0 0.0 0.0 1.0 0.0 32.0 176316.0]]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=2) # 80% training and 20% test\n",
    "\n",
    "print(\"\\nTraining Set:\")\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Training Set:\n",
      "[[ 1.73205081 -0.37796447 -0.37796447 -0.57735027 -0.57735027 -0.46773246\n",
      "   0.82835231]\n",
      " [-0.57735027  2.64575131 -0.37796447 -0.57735027 -0.57735027 -1.0730333\n",
      "   0.93565162]\n",
      " [-0.57735027 -0.37796447 -0.37796447 -0.57735027  1.73205081  1.51325208\n",
      "   1.20574353]\n",
      " [ 1.73205081 -0.37796447 -0.37796447 -0.57735027 -0.57735027 -0.8529239\n",
      "   0.30081741]\n",
      " [-0.57735027 -0.37796447 -0.37796447 -0.57735027  1.73205081 -1.0730333\n",
      "  -1.78701425]\n",
      " [-0.57735027 -0.37796447 -0.37796447  1.73205081 -0.57735027  0.41270511\n",
      "   0.29565926]\n",
      " [-0.57735027 -0.37796447 -0.37796447  1.73205081 -0.57735027  1.51325208\n",
      "  -0.76621194]\n",
      " [-0.57735027 -0.37796447  2.64575131 -0.57735027 -0.57735027  0.02751367\n",
      "  -1.01299795]]\n",
      "\n",
      "Scaled Test Set:\n",
      "[[-0.57735027 -0.37796447 -0.37796447  1.73205081 -0.57735027  0.63281451\n",
      "  -0.17715054]\n",
      " [-0.57735027 -0.37796447 -0.37796447  1.73205081 -0.57735027 -0.35767777\n",
      "  -1.59435484]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# assuming x_train and x_test are your training and test data\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"\\nScaled Training Set:\")\n",
    "print(X_train_scaled)\n",
    "\n",
    "print(\"\\nScaled Test Set:\")\n",
    "print(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
